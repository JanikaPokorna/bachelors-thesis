\chapter{Conjugate Gradient Method for Regular Systems}

\section{Motivation}

There are numerous ways of approaching the derivation of the Conjugate Gradient method, such as using the Lanczos method (see \cite{vanderVorst03}) or minimizing the quadratic functional \(\mathcal{F}(x)=\frac{1}{2}x^{T}Ax - x^{T}b\). We will be following the second approach, as it offers a basic geometric understanding when derived for \(A\in{\mathbb{R}^{nxn}}\) and \(b\in\mathbb{R}^n\), \(n\in{N}\). 

\indent To minimize a functional we wish to find its gradient and compare it to zero. Using the definition of gradient, let us compute 

\[\nabla{\mathbb{F}(x)} = \nabla{(\frac{1}{2}x^{T}Ax - x^{T}b)}\]

Linearity of the gradient gives us the following analysis (we look at the i-th component of the expression) 

\[\nabla{(\frac{1}{2}x^{T}Ax - x^{T}b)}=\frac{1}{2}[\nabla{(x^TAx)}]_{i} - [\nabla{(x^Tb)}]_i = \frac{1}{2}(\partial_i(x_kA_{kj}x_j)) - \partial_i(x_kb_k) =\]

\[= \frac{1}{2}(\delta_{ik}A_{kj}x_j + \delta_{ij}x_kA_{kj}) -\delta_{ik}x_k = \frac{1}{2}(A_{ij}x_j + x_kA_{ki}) - b_i \]


Which gives us 
\begin{equation}\label{1.1}
    \nabla\mathbb{F}(x)=2\frac{1}{2}Ax - b = Ax - b = 0.
\end{equation}

Minimizing the functional \(\mathbb{F}(x)\) thus gives us the solution to the system of equations \(Ax = b\). If we then assume \(A\) positive definite, such a solution \(x_*\) must be unique.


\section{Derivation of the Conjugate Gradient Method for Regular Systems}


Let us have a sparse positive-definite symmetric matrix \(A\in\mathbb{R}^{nxn} \) and a right-hand side vector \(b\in\mathbb{R}^n, n\in\mathbb{N} \) large. We  shall loosely follow the derivation from [\cite{Hnetykova12}]. For the purpose of having a better convergence rate than steepest descent, we shall be minimizing our functional using the \(\bold{"Energy\,Norm"}\), also called A-norm, which we define as 

\[\|x\|_A = \sqrt{(x,x)_A}\] for \(\forall{x}\in\mathbb{R}^n \), where we define the scalar product \[(x,y)_A = x^TAy \] for \(x,y\in\mathbb{R}^n  \).

This is indeed a norm if \(A \) is positive definite. Then it satisfies the three necessary conditions:
\begin{itemize}
    \item linearity
    \[\|\alpha x\|_A = \sqrt{\alpha x^TA\alpha x } = \sqrt{\alpha^2}\sqrt{x^TA x} = \alpha\|x\|_A, \]
    \item triangle inequality
    \[ (\|x\|_A+\|y\|_A)^2 = {\|x\|_A}^2 + 2\|x\|_A\|y\|_A +  {\|y\|_A}^2\geq {\|x\|_A}^2 + 2x^TAy +  {\|y\|_A}^2 \]
    \[ = x^TAx + 2x^TAy + y^TAy = (x+y)^TA(x+y) = {\|x+y\|_A}^2 \forall x,y \in\mathbb{R}^n\]
    using the Cauchy-Schwartz inequality, giving us \(\|x\|_A+\|y\|_A \geq \|x+y\|_A\)
    (note that \(A \) is symmetrical, so \(x^TAy = ((x^TA^T)y)^T = y^TAx \,,\, \forall x,y \in\mathbb{R}^n\)),
    \item positive-definiteness - stems trivially from positive-definiteness of matrix \(A\).
\end{itemize}

Let us assume we have an approximation \(x_k\) of the solution \(x_*\), and substitute it into the functional \(\mathbb{F}(x)\):
\begin{equation}
\begin{split}
\mathbb{F}(x_k) = \frac{1}{2}{x_k}^{T}Ax_k - {x_k}^{T}b = \frac{1}{2}{x_k}^{T}Ax_k - {x_k}^{T}Ax_* (\pm\frac{1}{2}{x_*}^{T}Ax_*) \\ = \frac{1}{2}{(x_*-x_k)}^{T}A(x_*-x_k) - \frac{1}{2}{x_*}^{T}Ax_* = \frac{1}{2}\|x_* - x_k\|_A-\frac{1}{2}\|x_*\|_A
\end{split}
\end{equation}
Since \(x_*\) is a constant value, to minimize the expression on the right we must find \(x_k\) such that \(\|x_* - x_k\|_A\) is minimal. Thus the minimization of the functional \(\mathbb{F}(x_k)\) is equivalent to minimizing the A-norm of \(x_* - x_k \).

Our goal of having a method with quick convergence leads us in the direction of numerical methods on Krylov subspaces (see \cite{LiesenStrakos12}). We would like to find each \(x_k\) from the k-th Krylov subspace \(\mathbb{K}_k(A,r_0), \)
\begin{equation}
r_0 =Ax_0 - b.
\end{equation}

If we had such an iterative method, we would know that the best approximation could be found in less than or exactly n steps.

Thus, our method would be an analogy of finding the least squares solution  
\begin{equation}\label{argmin}
    \operatorname{arg\,min}_{x_k \in x_0 + \mathbb{K}_k(A,r_0)} \|x_* - x_k\|_A,
\end{equation}
 for k = 1,2,...

To find each new \(x_k\) we want to find a correction of the previous approximation, i.e. a step of a certain length \(\gamma_{k-1}\) in a certain new direction \(p_{k-1}\). We get the equation
\begin{equation}\label{xk}
    x_k = x_{k-1} + \gamma_{k-1}p_{k-1},\indent k = 1,2,...
\end{equation}
and a question arises - how to choose \(\gamma_{k-1}\) and \(p_{k-1}\), so that we get our desired convergence characteristics.

Let us first focus on the factor \(\gamma\). Geometrically, we wish to find the ideal distance to travel from the starting point \(x_{k-1}\) in the direction of \(p(\gamma) = \gamma p_{k-1}\), so that we minimize the A-norm value of \(x_* - x_k\). Let us substitute \eqref{xk} into the expression
\[\|x_*-x_k\|_A
^2 = {\|x_*-x_{k-1} - \gamma_{k-1} p_{k-1}\|_A}^2 =\] 
\[= (x_*-x_{k-1} - \gamma_{k-1} p_{k-1})^TA(x_*-x_{k-1} - \gamma_{k-1} p_{k-1})=\] 
\[= (x_*-x_{k-1})^TA(x_*-x_{k-1}) - 2 \gamma_{k-1} p_{k-1}^TA(x_*-x_k) + \gamma_{k-1} ^2 p_{k-1}^TAp_{k-1}. \]

To minimize the right-hand side of this equation we find the derivative with respect to \(\gamma_{k-1}\). We get \[ 2p_{k-1}^TA(x_*-x_{k-1}) + 2\gamma_{k-1}p_{k-1}^TAp_{k-1}=0 \]
and it follows that
\[\gamma_{k-1} = \frac{p_{k-1}^TA(x_* - x_k)}{p_{k-1}^TAp_{k-1} } =\frac{p_{k-1}^T(Ax_* - Ax_k)}{p_{k-1}^TAp_{k-1} }=\frac{p_{k-1}^T(b - Ax_k)}{p_{k-1}^TAp_{k-1} }\]
\begin{equation}
    \gamma_{k-1} =\frac{p_{k-1}^Tr_{k-1}}{p_{k-1}^TAp_{k-1} }.
\end{equation}

To find \(r_k\), instead of computing it as \(b - Ax_k\) which would give us the method of steepest descent, we will take the previous residual vector \(r_{k-1}\) and combine it with \(p_{k-1}\) to get the new residual vector.
Let us take \eqref{xk} and multiply by \(A\) from the left side, and subtract both sides from the right-hand side vector b.
We get

\begin{equation}
\begin{split}
    Ax_k &= Ax_{k-1} + \gamma_{k-1}Ap_{k-1}\\
    b - Ax_k &= b -Ax_{k-1} - \gamma_{k-1}Ap_{k-1}
\end{split}  
\end{equation}

\begin{equation}
    r_k = r_{k-1} - \gamma_{k-1}Ap_{k-1}\\
\end{equation}

To generate vectors \(p_1,p_2,...,p_k\) we shall look at the error in steps 1,2...,k. 

\[x_*-x_k = x_* - x_k - \sum_{i=1}^{\infty}\gamma_{j-1} p_{j-1}\]

\[dopln\]

From the construction we can see, that \(p_i \perp p_j \forall i\neq j\). This lets us see, that we can reformulate our method as searching for the new approximation \(x_k\) minimal in the A-norm on the k-th Krylov space \(\mathcal{K}_k(A,r_0)\).

Using analysis of Krylov spaces, we can show that \(span{A} = span{K_k(A,ri)}\), which gives us that in exact arithmetic, our algorithm will always converge in N = span{A} steps or less.

\[ukaz\]

\newpage
This gives us the final algorithm:
\begin{algorithm}
\caption{Gonjugate Gradient Algorithm}
\begin{algorithmic}[1]
    \STATE \textbf{Input:} A, $\boldsymbol{b}$, $x_0$
    \STATE $r_0 := \boldsymbol{b} - Ax_0$
    \STATE $p_0 := r_0$
    \FOR{$k = 1, 2, \ldots$}
        \STATE $\gamma_{k-1} := \frac{r_{k-1}^T r_{k-1}}{p_{k-1}^T A p_{k-1}}$
        \STATE $x_k := x_{k-1} + \gamma_{k-1} p_{k-1}$
        \STATE $r_k := r_{k-1} - \gamma_{k-1} A p_{k-1}$
        \STATE $\delta_k := \frac{r_k^T r_k}{r_{k-1}^T r_{k-1}}$
        \STATE $p_k := r_k + \delta_k p_{k-1}$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

    

(to do - eigenvalues, spectral decomposition and ties to eigenvalues
- notation of solution, estimate, etc)
(Zmínit že meříme pomocí relativní chyby)